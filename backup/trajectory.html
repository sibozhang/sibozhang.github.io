<!DOCTYPE html>
<!-- htmlcs-disable -->
<!--[if lt IE 9]><html class="ie"><![endif]-->
<html lang="en">

<head>
    <link rel="import" href="/common/meta.html?__inline">
    <link rel="import" href="/common/css.html?__inline">
    <link rel="import" href="/common/js.html?__inline">


    <link rel="stylesheet" type="text/css" media="screen and (min-width: 750px)" href="/css/trajectory/index.css">
    <link rel="stylesheet" type="text/css" media="screen and (max-width: 750px)" href="/css/trajectory/m-index.css">

    <link rel="stylesheet" type="text/css" media="screen and (min-width: 750px)" href="/css/scene/github.css">
    <link rel="stylesheet" type="text/css" media="screen and (min-width: 750px)" href="/css/scene/scene.css">
    <style type="text/css">
    /* csshint-disable */
    .scene-body .content .github_table {
        border: unset;
        height: unset;
    }
    .markdown-body p,
    .markdown-body blockquote,
    .markdown-body ul,
    .markdown-body ol,
    .markdown-body dl,
    .markdown-body table,
    .markdown-body pre {
        font-size: 14px;
        color: #666;
    }
    .markdown-body .title-1 {
        font-size: 20px;
        color: #333;
    }
    .markdown-body .mt60 {
        margin-top: 60px !important;
    }
    .scene-body .content {
        padding-left: 25px;
    }
    .tip-top {
        word-wrap: normal;
    }
    ol li{
        list-style-type: decimal;
    }
    ul.submission-ul li{
        list-style-type: disc;
    }
    .word-1.indent {
        padding-left: 1.2em;
    }
    .content pre{
        padding: 16px;
        overflow: auto;
        font-size: 85%;
        line-height: 1.45;
        background-color: #f6f8fa;
        border-radius: 3px;
    }
    </style>
</head>

<body>
    <link rel="import" href="/common/header.html?__inline">

    <section class="banner main-wrapper scene-banner" id="banner-container">
    </section>
    <section class="main-wrapper">
        <div class="main scene-body" id="scene-container">
            <div class="menu fl"></div>
            <div class="content fl">
                <p class="title-1 mt60" id="to_data_href">1 · Introduction</p>
                <p class="word-1 mt20" id="to_collection_href">
                    Our trajectory dataset consists of camera-based images, LiDAR scanned point clouds, and manually annotated trajectories. It is collected under various lighting conditions and traffic densities. More specifically, it contains highly complicated traffic flows mixed with vehicles, riders, and pedestrians.                 </p>
                <div class="fl mt20 demo">
                    <p>
                        <img src="./public/img/trajectory/1.gif" width="260px">
                        <img src="./public/img/trajectory/2.gif" width="260px">
                        <img src="./public/img/trajectory/3.gif" width="260px">
                    </p>
                    <p>
                        <img src="./public/img/trajectory/4.gif" width="260px">
                        <img src="./public/img/trajectory/5.gif" width="260px">
                        <img src="./public/img/trajectory/6.gif" width="260px">
                    </p>
                </div>
                <div class="cb"></div>
                <p class="title-1 mt60" id="to_download_href">2 · Data Download</p>
                <p class="word-1 mt20">
                    The trajectory dataset consists of 53min training sequences and 50min testing sequences captured at 2 frames per second.
                </p>
                <p class="title-2 mt20">Sample data</p>
                <div class="down_list mt10 clearfix">
                    <a class="down_btn fl mr10" value="sample_trajectory">sample_trajectory.zip</a>
                </div>
                <div class="down_list mt20 clearfix">
                    <a class="down_btn fl mr10" value="sample_image">sample_image.zip</a>
                </div>
                <p class="title-2 mt20">Training data</p>
                <div class="down_list mt20 clearfix">
                    <a class="down_btn fl mr10" value="prediction_train.zip">prediction_train.zip</a>
                </div>
                <p class="title-2 mt20">Testing data</p>
                <div class="down_list mt20 clearfix">
                    <a class="down_btn fl mr10" value="prediction_test.zip">prediction_test.zip</a>
                </div>


                <div class="cb"></div>


                <div class="cb"></div>
                <p class="title-1 mt60" id="to_structure_href">3 · Data Structure</p>
                <p class="word-1 mt10">
                    The folder structure of the trajectory prediction is as follows:
                </p>
                <p class="word-1 mt10">
                    1) prediction_train.zip: training data for trajectory prediction.<br>
                    ∙ Each file is a 1min sequence with 2fps.<br>
                    ∙ Each line in a file contains frame_id, object_id, object_type, position_x, position_y, position_z, object_length, object_width, object_height, heading.<br>
                    ∙ There are five different object types as shown in following table. During the evaluation in this challenge, we treat the first two types, small vehicle and big vehicle, as one type (vehicle).
                </p>
                <div class="markdown-body">
                <table class="github_table">
                    <thead>
                        <tr>
                            <th>object_type</th>
                            <th align="center">small vehicles</th>
                            <th align="center">big vehicles</th>
                            <th align="center">pedestrian</th>
                            <th align="center">motorcyclist and bicyclist</th>
                            <th align="center">others</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>ID</td>
                            <td align="center">1</td>
                            <td align="center">2</td>
                            <td align="center">3</td>
                            <td align="center">4</td>
                            <td align="center">5</td>
                        </tr>
                    </tbody>
                </table>
            </div>
                <p class="word-1 mt10">∙ Position is given in the world coordinate system. The unit for the position and bounding box is meter.<br>
                    ∙ The heading value is the steering radian with respect to the direction of the object.<br>
                    ∙ In this challenge, we mainly evaluate predicted position_x and position_y in the next 3 seconds.
                </p>
                <p class="word-1 mt10">
                    2) prediction_test.zip: testing data for trajectory prediction.<br>
                    ∙ Each line contains frame_id, object_id, object_type, position_x, position_y, position_z, object_length, object_width, object_height, heading.<br>
                    ∙ A testing sequence contains every six frames in the prediction_test.txt. Each sequence is evaluated independently.
                </p>
                <p class="title-1 mt60" id="to_evaluation_href">4 · Evaluation</p>
                <p class="word-1 mt20">
                    The evaluation scripts are released on github: <a href="https://github.com/sibozhang/dataset-api/tree/master/trajectory_prediction">https://github.com/ApolloScapeAuto/dataset-api/tree/master/trajectory_prediction</a>.
                </p>
                <p class="word-1 mt20">
                    During the evaluation in this challenge, we treat the first two types, small vehicle and big vehicle, as one type (vehicle). However, please keep the original type IDs during the training and prediction, we will merge the first two types in our evaluation scripts. In this challenge, the data from the first three seconds in each sequence is given as input data, the task is to predict trajectories of objects for the next three seconds. The objects used in evaluation are the objects that appear in the last frame of the first three seconds. The errors between predicted locations and the ground truth of these objects are then computed.
                </p>
                <div class="cb"></div>
                <p class="title-1 mt60" id="to_metric_href">5 · Metric formula</p>
                <p class="word-1 mt10 markdown-body">
                    We adopt the metric similar to [1] to measure the performance of algorithms.
                </p>
                <p class="word-1 mt10 markdown-body ">
                    1. Average displacement error (ADE): The mean Euclidean distance over all the predicted positions and ground truth positions during the prediction time.
                </p>
                <p class="word-1 mt10 markdown-body ">
                    2. Final displacement error (FDE): The mean Euclidean distance between the final predicted positions and the corresponding ground truth locations.
                    <br>
                    Because the trajectories of cars, bicyclist and pedestrians have different scales, we use the following weighted sum of ADE (WSADE) and weighted sum of FDE (WSFDE) as metrics.
                </p>
                <div class="cb"></div>
                <img src="public/img/scene/prediction1.png" align="middle" width="300px" style="max-width:100%">
                <p class="word-1 mt10 markdown-body" >
                    where
                    <img class="spce-icon" src="public/img/scene/prediction_img/DV.png" align="middle" width="12.067218899999991pt" height="14.15524440000002pt/" style="max-width:100%">,
                    <img class="spce-icon extra" src="public/img/scene/prediction_img/DP.png" align="middle" width="12.067218899999991pt" height="14.15524440000002pt/" style="max-width:100%">,
                    and
                    <img class="spce-icon" src="public/img/scene/prediction_img/DB.png" align="middle" width="12.067218899999991pt" height="14.15524440000002pt/" style="max-width:100%">
                    are related to reciprocals of the average velocity of vehicles, pedestrian and cyclist in the dataset. We adopt 0.20, 0.58, 0.22 respectively.
                </p>
                <div class="cb"></div>
                <p class="title-1 mt60" id="to_rules_href">6 · Rules of ranking</p>
                <p class="word-1 mt10">Result benchmark will be:</p>
                <div class="markdown-body">
                    <table class="github_table">
                        <thead>
                            <tr>
                                <th>Rank</th>
                                <th>Method</th>
                                <th align="center">WSADE</th>
                                <th align="center">ADEv</th>
                                <th align="center">ADEp</th>
                                <th align="center">ADEb</th>
                                <th align="center">WSFDE</th>
                                <th align="center">FDEv</th>
                                <th align="center">FDEp</th>
                                <th align="center">FDEb</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td align="center">xx</td>
                                <td>xxx</td>
                                <td align="center">xx</td>
                                <td align="center">xx</td>
                                <td align="center">xx</td>
                                <td align="center">xx</td>
                                <td align="center">xx</td>
                                <td align="center">xx</td>
                                <td align="center">xx</td>
                                <td align="center">xx</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <p class="word-1 mt10">Our ranking will determined by WSADE of all types of objects.</p>
                <div class="cb"></div>
                <p class="title-1 mt60" id="to_format_href">7 · Format of submission file</p>
                <p class="word-1 mt10">
                    - The submission should be one single text file.
                    <br>
                    - Each submission line should represent one object instance, with the following fields: frame_id, object_id, object_type, position_x, and position_y. Each row in submission must have all the required fields with the exact order.
                    <br>
                    - Every six frames constitute a sequence. Pay attention to make right correspondence to test data. It means sequences in test data and your result should have the same number and same order. Same objects should have the same id. Different frames should have different ids.                </p>
                <div class="cb"></div>
                <div class="eccv">
                    <a class="task-submission-btn submit_achievement">
                        Participate
                    </a>
                    <a class="task-submission-btn leader_board" href="/leader_board.html" target="_Blank">
                        LeaderBoard
                    </a>
                </div>
                <div class="cb"></div>
                <p class="title-1 mt60" id="to_publicat_href">8 · Publication</p>
                <table style="width:100%" cellspacing="10" valign="top">
                    <tr>
                    <td valign="top" align='left'>
                    <p>Please cite our paper in your publications if our dataset is used in your research.</p >
                    <p>TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents [<a href="http://gamma.cs.unc.edu/TPredict/TrafficPredict.html"><font color="red">Website</a></font>][<a href="https://arxiv.org/pdf/1811.02146.pdf"><font color="red">PDF</a></font>] <a href="https://ad-apolloscape.cdn.bcebos.com/TrafficPredict/trafficpredict_bibtex.txt">[BibTex]</a>
                      <br/>
                      Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wenping Wang, and Dinesh Manocha.<br> <em>AAAI(oral), 2019</em><br><br>
                    </tr>
                </table>
                <div class="cb"></div>
                <p class="title-1 mt60" id="to_reference_href">9 · Reference</p>
                <p class="word-1 mt10">
                    [1] Pellegrini S, Ess A, Schindler K, et al. You'll never walk alone: Modeling social behavior for multi-target tracking[C]. Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009: 261-268.
                </p>


                <div class="cb"></div>
                <article class="markdown-body entry-content" itemprop="text">
                    <p class="title-1 mt60" id="to_contact_href">Contact</p>
                    <p>Please feel free to contact us with any questions and submit issue on github:</p>
                    <p><a href="https://github.com/ApolloScapeAuto/dataset-api/tree/master/trajectory_prediction">Github</a></p>
                </article>

                <div class="cb"></div>
                <p class="title-1 mt60" id="to_reference_href">Q & A</p>
                <p class="word-1 mt10">
                    Q1. Does the dataset include synchronized RGB data?
                </p>
                <p class="word-1 mt10">
                    We have not labeled the image data. Current challenge is just based on the trajectory data.
                </p>
                <p class="word-1 mt10">
                    Q2. Is the trajectory of the ego vehicle also included?
                </p>
                <p class="word-1 mt10">
                    No, the data does not contain the trajectory of the ego vehicle.
                </p>
                <p class="word-1 mt10">
                    Q3. How are these world coordinates generated?
                </p>
                <p class="word-1 mt10">
                    We use the relative positions from LiDAR and the GPS of the ego vehicle to compute the locations of other traffic-agents in the world coordinate system.
                </p>
                <p class="word-1 mt10">
                    Q4. What are the relationships among different files in the training dataset?
                </p>
                <p class="word-1 mt10">
                    They are captured in different period and they are independent.
                </p>
                <div class="mt60" style="font-size: 18px;text-align: center;">The dataset we released is &nbsp;desensitized street view for academic use only.</div>
            </div>
            <div class="cb"></div>

        </div>
    </section>
    <div class="protocol-bg" id="protocol"></div>
    <div class="protocol_container" id="protocol_container">
    </div>
    <div class="done_load"></div>
    <script type="text/javascript" src="/js/dataset_trajectory.js"></script>

    <link rel="import" href="/common/footer.html?__inline">



</body>

</html>
